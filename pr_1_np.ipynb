{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data('train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100000, 100001, 100002, ..., 349997, 349998, 349999])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data\n",
    "1. add bias in data\n",
    "2. split column \"mn â€™PRI jet num'\n",
    "3. replace -999 with average of whole train\n",
    "4. split data 0.8:0.2\n",
    "5. normalize\n",
    "6. balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.c_[np.ones((input_data.shape[0],1)),input_data]\n",
    "jet_num = np.zeros((len(data),4))\n",
    "for i in range(4):\n",
    "    row = np.where(data[:,23] == i)[0]\n",
    "    jet_num[row,i] = 1\n",
    "data[:,:0]\n",
    "data = np.c_[data[:,:23],data[:,24:]]\n",
    "data = np.c_[data,jet_num]\n",
    "# data[:5,21:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.where(data == -999)\n",
    "data[data == -999] = np.nan\n",
    "c_mean = np.nanmean(data,axis=0)\n",
    "data[np.isnan(data)] = c_mean[pos[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb[yb>0] = 0\n",
    "yb[yb<0] = 1\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = 0;b = 1 at first\n",
    "# s = 1;b = -1 now from the helpers\n",
    "num_samples = data.shape[0]\n",
    "\n",
    "fraction_train = 0.8\n",
    "np.random.seed(0)\n",
    "rinds = np.random.permutation(num_samples)\n",
    "\n",
    "d_train = data[rinds[:int(num_samples * fraction_train)]] \n",
    "yb_train = yb[rinds[:int(num_samples * fraction_train)]]  \n",
    "\n",
    "d_test = data[rinds[int(num_samples * fraction_train):]] \n",
    "yb_test = yb[rinds[int(num_samples * fraction_train):]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        , 128.224     ,  76.169     , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  1.        , 121.85852836,  79.879     , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  1.        , 103.989     ,  60.866     , ...,   1.        ,\n",
       "          0.        ,   0.        ],\n",
       "       ...,\n",
       "       [  1.        , 121.85852836,  67.554     , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  1.        , 125.188     ,  88.344     , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  1.        ,  72.741     ,  95.563     , ...,   1.        ,\n",
       "          0.        ,   0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize\n",
    "mean = np.mean(d_test, axis=0)\n",
    "sdt = np.std(d_test)\n",
    "data_train = (d_train - mean) / sdt\n",
    "data_test = ( d_test - mean) / sdt\n",
    "\n",
    "#problem imbalance classes\n",
    "#add additional weight to features which sample was represented in the minority\n",
    "#np.count_nonzero(labels == 0) # 85667 - multiply weight for these samples to 2\n",
    "#np.count_nonzero(labels == 1) # 164333 - multiply weight for these samples to 1\n",
    "\n",
    "imbalance = yb_train.copy()\n",
    "imbalance[np.where(imbalance == 0)] = 2\n",
    "imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(pred):\n",
    "    \"\"\" Sigmoid function\n",
    "    \n",
    "    Args:\n",
    "        pred (np.array): Input data of shape (N, ) \n",
    "        \n",
    "    Returns:\n",
    "        np.array: Probabilites of shape (N, ), where each value is in [0, 1].\n",
    "    \"\"\"\n",
    "    pos = pred >= 0\n",
    "    neg = pred < 0\n",
    "    pred[pos] = 1 / (1 + np.exp(-pred[pos]))\n",
    "    pred[neg] = 1 - 1 / (1 + np.exp(pred[neg]))\n",
    "    return pred\n",
    "\n",
    "def loss_logistic(data, labels, w, alpha): \n",
    "    \"\"\" Logistic regression loss function for binary classes\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, )\n",
    "        alpha (int) : regularization factor\n",
    "    Returns:\n",
    "        int: Loss of logistic regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.mean((-np.log(sigmoid(-data@w)) - labels*data.dot(w))*imbalance) + (alpha/2)*np.linalg.norm(w)**2\n",
    "\n",
    "def logistic_regression_classify(data, w):\n",
    "    \"\"\" Classification function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, )\n",
    "    Returns:\n",
    "        np.array: Label assignments of data of shape (N, )\n",
    "    \"\"\"\n",
    "    #### find predictions and threshold.\n",
    "    predictions = sigmoid(data.dot(w)) \n",
    "    predictions[predictions<0.5]=0\n",
    "    predictions[predictions>=0.5]=1        \n",
    "    return predictions\n",
    "\n",
    "def gradient_logistic(data, labels, w, alpha):\n",
    "    \"\"\" Logistic regression gradient function for binary classes\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, )\n",
    "        alpha (int) : regularization factor\n",
    "    Returns:\n",
    "        np. array: Gradient array of shape (D, )\n",
    "    \"\"\"\n",
    "    return data.T.dot((sigmoid(data.dot(w))-labels)*imbalance) / data.shape[0] + alpha*w\n",
    "\n",
    "def accuracy(labels_tr, labels_pred):\n",
    "    \"\"\" Computes accuracy.\n",
    "    \n",
    "    Args:\n",
    "        labels_gt (np.array): GT labels of shape (N, ).\n",
    "        labels_pred (np.array): Predicted labels of shape (N, ).\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy, in range [0, 1].\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sum(labels_tr == labels_pred) / labels_tr.shape[0]\n",
    "\n",
    "def logistic_regression_train(data, labels, max_iters=10, lr=0.001, alpha=1e-3):\n",
    "    \"\"\" Training function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        max_iters (integer): Maximum number of iterations. Default:10\n",
    "        lr (float): The learning rate of  the gradient step. Default:0.001\n",
    "        alpha (int) : regularization factor\n",
    "        \n",
    "    Returns:\n",
    "        np.array: weights of shape(D, )\n",
    "    \"\"\"\n",
    "\n",
    "    #initialize the weights randomly according to a Gaussian distribution\n",
    "    weights = np.random.normal(0., 0.01, [data.shape[1],])\n",
    "    for it in range(max_iters):\n",
    "        ########## find gradient and do a gradient step\n",
    "        xx = data @ weights\n",
    "        gradient = gradient_logistic(data, labels, weights,alpha)\n",
    "        weights = weights - lr*gradient\n",
    "        ##################################\n",
    "        predictions = logistic_regression_classify(data, weights)\n",
    "        print(f'loss: {loss_logistic(data, labels, weights,alpha) : .5f}, acc: {accuracy(labels, predictions): .5f}')\n",
    "        \n",
    "    return weights, loss_logistic(data, labels, weights,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.92544, acc:  0.62918\n",
      "loss:  0.92538, acc:  0.62938\n",
      "loss:  0.92531, acc:  0.62971\n",
      "loss:  0.92525, acc:  0.62993\n",
      "loss:  0.92518, acc:  0.63019\n",
      "loss:  0.92512, acc:  0.63047\n",
      "loss:  0.92505, acc:  0.63078\n",
      "loss:  0.92499, acc:  0.63108\n",
      "loss:  0.92493, acc:  0.63128\n",
      "loss:  0.92486, acc:  0.63158\n",
      "loss:  0.92480, acc:  0.63187\n",
      "loss:  0.92473, acc:  0.63206\n",
      "loss:  0.92467, acc:  0.63228\n",
      "loss:  0.92461, acc:  0.63239\n",
      "loss:  0.92454, acc:  0.63260\n",
      "loss:  0.92448, acc:  0.63287\n",
      "loss:  0.92442, acc:  0.63315\n",
      "loss:  0.92435, acc:  0.63339\n",
      "loss:  0.92429, acc:  0.63357\n",
      "loss:  0.92423, acc:  0.63377\n",
      "loss:  0.92417, acc:  0.63396\n",
      "loss:  0.92410, acc:  0.63408\n",
      "loss:  0.92404, acc:  0.63428\n",
      "loss:  0.92398, acc:  0.63462\n",
      "loss:  0.92392, acc:  0.63475\n",
      "loss:  0.92386, acc:  0.63496\n",
      "loss:  0.92379, acc:  0.63513\n",
      "loss:  0.92373, acc:  0.63516\n",
      "loss:  0.92367, acc:  0.63539\n",
      "loss:  0.92361, acc:  0.63552\n",
      "loss:  0.92355, acc:  0.63567\n",
      "loss:  0.92349, acc:  0.63579\n",
      "loss:  0.92343, acc:  0.63599\n",
      "loss:  0.92337, acc:  0.63631\n",
      "loss:  0.92331, acc:  0.63653\n",
      "loss:  0.92324, acc:  0.63665\n",
      "loss:  0.92318, acc:  0.63692\n",
      "loss:  0.92312, acc:  0.63713\n",
      "loss:  0.92306, acc:  0.63730\n",
      "loss:  0.92300, acc:  0.63741\n",
      "loss:  0.92294, acc:  0.63766\n",
      "loss:  0.92288, acc:  0.63791\n",
      "loss:  0.92283, acc:  0.63806\n",
      "loss:  0.92277, acc:  0.63821\n",
      "loss:  0.92271, acc:  0.63836\n",
      "loss:  0.92265, acc:  0.63848\n",
      "loss:  0.92259, acc:  0.63861\n",
      "loss:  0.92253, acc:  0.63875\n",
      "loss:  0.92247, acc:  0.63883\n",
      "loss:  0.92241, acc:  0.63907\n",
      "loss:  0.92235, acc:  0.63913\n",
      "loss:  0.92229, acc:  0.63929\n",
      "loss:  0.92224, acc:  0.63946\n",
      "loss:  0.92218, acc:  0.63971\n",
      "loss:  0.92212, acc:  0.63983\n",
      "loss:  0.92206, acc:  0.63998\n",
      "loss:  0.92200, acc:  0.64007\n",
      "loss:  0.92195, acc:  0.64030\n",
      "loss:  0.92189, acc:  0.64050\n",
      "loss:  0.92183, acc:  0.64065\n",
      "loss:  0.92177, acc:  0.64081\n",
      "loss:  0.92172, acc:  0.64099\n",
      "loss:  0.92166, acc:  0.64108\n",
      "loss:  0.92160, acc:  0.64121\n",
      "loss:  0.92155, acc:  0.64141\n",
      "loss:  0.92149, acc:  0.64150\n",
      "loss:  0.92143, acc:  0.64158\n",
      "loss:  0.92138, acc:  0.64163\n",
      "loss:  0.92132, acc:  0.64173\n",
      "loss:  0.92127, acc:  0.64190\n",
      "loss:  0.92121, acc:  0.64199\n",
      "loss:  0.92115, acc:  0.64212\n",
      "loss:  0.92110, acc:  0.64221\n",
      "loss:  0.92104, acc:  0.64236\n",
      "loss:  0.92099, acc:  0.64255\n",
      "loss:  0.92093, acc:  0.64278\n",
      "loss:  0.92088, acc:  0.64290\n",
      "loss:  0.92082, acc:  0.64297\n",
      "loss:  0.92077, acc:  0.64307\n",
      "loss:  0.92071, acc:  0.64328\n",
      "loss:  0.92066, acc:  0.64338\n",
      "loss:  0.92060, acc:  0.64341\n",
      "loss:  0.92055, acc:  0.64353\n",
      "loss:  0.92049, acc:  0.64362\n",
      "loss:  0.92044, acc:  0.64375\n",
      "loss:  0.92038, acc:  0.64398\n",
      "loss:  0.92033, acc:  0.64409\n",
      "loss:  0.92027, acc:  0.64414\n",
      "loss:  0.92022, acc:  0.64418\n",
      "loss:  0.92017, acc:  0.64426\n",
      "loss:  0.92011, acc:  0.64433\n",
      "loss:  0.92006, acc:  0.64442\n",
      "loss:  0.92001, acc:  0.64448\n",
      "loss:  0.91995, acc:  0.64464\n",
      "loss:  0.91990, acc:  0.64475\n",
      "loss:  0.91985, acc:  0.64489\n",
      "loss:  0.91979, acc:  0.64494\n",
      "loss:  0.91974, acc:  0.64502\n",
      "loss:  0.91969, acc:  0.64513\n",
      "loss:  0.91964, acc:  0.64524\n"
     ]
    }
   ],
   "source": [
    "weights, loss = logistic_regression_train(data_train, yb_train, max_iters=100, lr=5e-4,alpha=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00201736, -0.00502268,  0.01094692, -0.01781726, -0.02185558,\n",
       "        -0.01154768, -0.02055961,  0.00703959,  0.00636955,  0.01129834,\n",
       "        -0.02018703,  0.00267791, -0.00360718,  0.0052221 ,  0.01782481,\n",
       "        -0.00438853, -0.00424051,  0.00963502,  0.00152777, -0.00694184,\n",
       "         0.00103654,  0.013164  , -0.01991231,  0.00703761,  0.02435765,\n",
       "        -0.01250688,  0.02185152, -0.0025418 ,  0.00755125,  0.00844345,\n",
       "         0.01914765, -0.01168945,  0.00505958,  0.00237452]),\n",
       " 0.9196352490949115)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb_te, data_test, ids_te = load_csv_data('test.csv', sub_sample=False)\n",
    "test = np.c_[np.ones((data_test.shape[0],1)),data_test]\n",
    "jet_num = np.zeros((len(test),4))\n",
    "for i in range(4):\n",
    "    row = np.where(test[:,23] == i)[0]\n",
    "    jet_num[row,i] = 1\n",
    "test = np.c_[test[:,:23],test[:,24:]]\n",
    "test = np.c_[test,jet_num]\n",
    "pos = np.where(test == -999)\n",
    "test[test == -999] = np.nan\n",
    "c_mean = np.nanmean(test,axis=0)\n",
    "test[np.isnan(test)] = c_mean[pos[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 34)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test = (test - mean) / sdt\n",
    "test_predict = logistic_regression_classify(norm_test, weights)\n",
    "test_predict[test_predict == 1] = -1\n",
    "test_predict[test_predict == 0] = 1\n",
    "create_csv_submission(ids_te, test_predict, \"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9479987f2ed20999a713725a6756863f6b1eb46c4b0caa63864f6802c4dcff56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
